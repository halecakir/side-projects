{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "from common import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentParser:\n",
    "    permission_type = \"READ_CONTACTS\"\n",
    "    train = \"/home/huseyinalecakir/Security/data/acnet-data/ACNET_DATASET.csv\"\n",
    "    train_file_type = \"acnet\"\n",
    "    external_embedding = \"/home/huseyinalecakir/Security/data/pretrained-embeddings/{}\".format(\"scraped_with_porter_stemming_300.bin\")\n",
    "    external_embedding_type = \"word2vec\"\n",
    "    stemmer = \"porter\"\n",
    "    saved_parameters_dir = \"/home/huseyinalecakir/Security/data/saved-parameters\"\n",
    "    saved_prevectors    = \"embeddings.pickle\"\n",
    "    saved_vocab_train = \"acnet-vocab.txt\"\n",
    "    saved_all_data = \"{}/all_data\".format(saved_parameters_dir)\n",
    "    reviews = \"/home/huseyinalecakir/Security/data/reviews/acnet-reviews/acnet_initial/app_reviews_original.csv\"\n",
    "    lower = True\n",
    "    outdir = \"./test/{}\".format(permission_type)\n",
    "\n",
    "class TorchOptions:\n",
    "    embedding_size = 300\n",
    "    hidden_size = 300\n",
    "    output_size = 1\n",
    "    init_weight = 0.08\n",
    "    decay_rate = 0.985\n",
    "    learning_rate = 0.0001\n",
    "    plot_every = 2500\n",
    "    print_every = 50\n",
    "    grad_clip = -1\n",
    "    dropout = 0\n",
    "    dropoutrec = 0\n",
    "    learning_rate_decay = 1 #0.985\n",
    "    learning_rate_decay_after = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, w2i):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.w2i = w2i\n",
    "        \n",
    "        self.embedding = None\n",
    "        self.lstm = nn.LSTM(self.opt.embedding_size, self.opt.hidden_size, batch_first=True)\n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "        #self.__initParameters()\n",
    "        \n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "                #init.xavier_normal()\n",
    "\n",
    "    def initalizedPretrainedEmbeddings(self, embeddings):\n",
    "        weights_matrix = np.zeros(((len(self.w2i), self.opt.hidden_size)))\n",
    "\n",
    "        for word in self.w2i:\n",
    "            weights_matrix[self.w2i[word]] = embeddings[word]\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "\n",
    "    def forward(self, input_src):\n",
    "        src_emb = self.embedding(input_src) # batch_size x src_length x emb_size\n",
    "        if self.opt.dropout > 0:\n",
    "            src_emb = self.dropout(src_emb)\n",
    "        outputs, (h,c) = self.lstm(src_emb)\n",
    "        return outputs, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size =  opt.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, opt.output_size)\n",
    "\n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def forward(self, prev_h):\n",
    "        if self.opt.dropout > 0:\n",
    "            prev_h = self.dropout(prev_h)\n",
    "        h2y = self.linear(prev_h)\n",
    "        pred = self.sigmoid(h2y)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_item(opt, args, sentence, encoder, classifier, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, (hidden,cell) =  encoder(sentence.index_tensor)\n",
    "\n",
    "    pred = classifier(hidden)\n",
    "\n",
    "    loss = criterion(pred, torch.tensor([[[sentence.permissions[args.permission_type]]]], dtype=torch.float))\n",
    "    loss.backward()\n",
    "    \n",
    "    if opt.grad_clip != -1:\n",
    "        torch.nn.utils.clip_grad_value_(encoder.parameters(),opt.grad_clip)\n",
    "        torch.nn.utils.clip_grad_value_(classifier.parameters(),opt.grad_clip)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def predict(opt, sentence, encoder, classifier):\n",
    "    outputs, (hidden,cell) = encoder(sentence.index_tensor)\n",
    "    pred = classifier(hidden)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_roc_auc(predictions, gold):\n",
    "    y_true = np.array(gold)\n",
    "    y_scores = np.array(predictions)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    pr_auc = average_precision_score(y_true, y_scores)\n",
    "    return roc_auc, pr_auc\n",
    "\n",
    "def train_and_test(opt, args, w2i, train_data, test_data, ext_embeddings):\n",
    "    encoder = Encoder(opt, w2i)\n",
    "    encoder.initalizedPretrainedEmbeddings(ext_embeddings)\n",
    "    \n",
    "    classifier = Classifier(opt)\n",
    "\n",
    "    params = list(encoder.parameters()) + list(classifier.parameters())\n",
    "    optimizer = optim.Adam(params)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    print(\"Training...\")\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    for index, sentence in enumerate(train_data):\n",
    "        loss = train_item(opt, args, sentence, encoder, classifier, optimizer, criterion)\n",
    "        if index != 0:\n",
    "            if index % opt.print_every == 0:\n",
    "                print(\"Index {} Loss {}\".format(index,np.mean(losses[index-opt.print_every:])))\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "       \n",
    "    print(\"Predicting..\")\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    predictions = []\n",
    "    gold = []\n",
    "    with torch.no_grad():\n",
    "        for index, sentence in enumerate(test_data):\n",
    "            pred = predict(opt, sentence, encoder, classifier)\n",
    "            predictions.append(pred)\n",
    "            gold.append(sentence.permissions[args.permission_type])\n",
    "    \n",
    "    return pr_roc_auc(predictions, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "args = ArgumentParser()\n",
    "opt = TorchOptions()\n",
    "\n",
    "save_dir = os.path.join(args.saved_all_data, \"without_prediction.pickle\")\n",
    "ext_embeddings, reviews, sentences, w2i = load_data(save_dir)\n",
    "sentences = [s for s in sentences if len(s.preprocessed_sentence)>0] #remove sentences with 0 length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/huseyinalecakir/Security/data/saved-parameters/saved-data/emdeddings-sentences-w2i.pickle\", [ext_embeddings, sentences, w2i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/home/huseyinalecakir/Security/data/saved-parameters/saved-data/reviews.pickle\", reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsl = load_data(\"/home/huseyinalecakir/Security/data/saved-parameters/saved-data/reviews.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_embeddings_, sentences_, w2i_ = load_data(\"/home/huseyinalecakir/Security/data/saved-parameters/saved-data/emdeddings-sentences-w2i.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = np.array(sentences)\n",
    "random.shuffle(documents)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "roc_l, pr_l = [], []\n",
    "for foldid, (train, test) in enumerate(kfold.split(documents)):\n",
    "    print(\"Fold {}\".format(foldid))\n",
    "    train_data = documents[train]\n",
    "    test_data = documents[test]\n",
    "    roc, pr = train_and_test(opt, args, w2i, train_data, test_data, ext_embeddings)\n",
    "    print(\"ROC {} PR {}\".format(roc, pr))\n",
    "    roc_l.append(roc)\n",
    "    pr_l.append(pr)\n",
    "\n",
    "print(\"Summary : ROC {} PR {}\".format(np.mean(roc_l), np.mean(pr_l)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
