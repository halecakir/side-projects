{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "class ArgumentParser():\n",
    "    permission_type = \"READ_CONTACTS\"\n",
    "    train = \"/home/huseyinalecakir/Security/data/acnet-data/ACNET_DATASET.csv\"\n",
    "    train_file_type = \"acnet\"\n",
    "    external_embedding = \"/home/huseyinalecakir/Security/data/pretrained-embeddings/{}\".format(\"scraped_with_porter_stemming_300.bin\")\n",
    "    external_embedding_type = \"word2vec\"\n",
    "    stemmer = \"porter\"\n",
    "    saved_parameters_dir = \"/home/huseyinalecakir/Security/data/saved-parameters/\"\n",
    "    saved_prevectors    = \"embeddings.pickle\"\n",
    "    saved_vocab_train = \"acnet-vocab.txt\"\n",
    "    lower = True\n",
    "    outdir = \"./test/{}\".format(permission_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchOptions():\n",
    "    def __init__(self):\n",
    "        self.rnn_size = 300\n",
    "        self.init_weight = 0.08\n",
    "        self.decay_rate = 0.985\n",
    "        self.learning_rate = 0.0001\n",
    "        self.plot_every = 2500\n",
    "        self.print_every = 2500\n",
    "        self.grad_clip = 5\n",
    "        self.dropout = 0\n",
    "        self.dropoutrec = 0\n",
    "        self.learning_rate_decay = 0.985\n",
    "        self.learning_rate_decay_after = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceReport:\n",
    "    \"\"\"TODO\"\"\"\n",
    "    def __init__(self, id, sentence, mark):\n",
    "        self.app_id = id\n",
    "        self.mark = mark\n",
    "        self.preprocessed_sentence = None\n",
    "        self.sentence = sentence\n",
    "        self.prediction_result = None\n",
    "        self.index_tensors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_row_acnet(infile, gold_permission, stemmer, embeddings):\n",
    "    print(\"Loading row {} \".format(infile))\n",
    "    #read training data\n",
    "    print(\"Reading Train Sentences\")\n",
    "    tagged_train_file = pd.read_csv(infile)\n",
    "    train_sententence_reports = []\n",
    "    acnet_map = {\"RECORD_AUDIO\" : \"MICROPHONE\", \"READ_CONTACTS\": \"CONTACTS\", \"READ_CALENDAR\": \"CALENDAR\", \"ACCESS_FINE_LOCATION\" : \"LOCATION\" ,\n",
    "    \"CAMERA\" : \"CAMERA\", \"READ_SMS\" : \"SMS\", \"READ_CALL_LOGS\" : \"CALL_LOG\", \"CALL_PHONE\" : \"PHONE\" , \"WRITE_SETTINGS\" : \"SETTINGS\" ,\n",
    "    \"GET_TASKS\" : \"TASKS\"}\n",
    "    for idx, row in tagged_train_file.iterrows():\n",
    "        app_id = row[\"app_id\"]\n",
    "        sentence = row[\"sentence\"]\n",
    "        mark = row[acnet_map[gold_permission]]\n",
    "        sentence_report = SentenceReport(app_id, sentence, mark)\n",
    "        preprocessed = NLPUtils.preprocess_sentence(sentence_report.sentence, stemmer)\n",
    "        sentence_report.preprocessed_sentence = [word for word in preprocessed if word in embeddings]\n",
    "        train_sententence_reports.append(sentence_report)\n",
    "    print(\"Loading completed\")\n",
    "    return train_sententence_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acnet_app_ids(infile):\n",
    "    app_ids = []\n",
    "    with open(APPLIST_FILE) as target:\n",
    "        for line in target:\n",
    "            app_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.i2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        self.h2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        if opt.dropoutrec > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropoutrec)\n",
    "            \n",
    "    def forward(self, x, prev_c, prev_h):\n",
    "        gates = self.i2h(x) + self.h2h(prev_h)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        if self.opt.dropoutrec > 0:\n",
    "            cellgate = self.dropout(cellgate)\n",
    "        cy = (forgetgate * prev_c) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)  # n_b x hidden_dim\n",
    "        return cy, hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, w2i, ext_embeddings):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.w2i = w2i\n",
    "        self.hidden_size = opt.rnn_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(w2i), self.hidden_size)\n",
    "        self.lstm = LSTM(self.opt)\n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.__initParameters()\n",
    "        self.__initalizedPretrainedEmbeddings(ext_embeddings)\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -opt.init_weight, opt.init_weight)\n",
    "                \n",
    "    def __initalizedPretrainedEmbeddings(self, embeddings):\n",
    "        weights_matrix = np.zeros(((len(self.w2i), self.hidden_size)))\n",
    "        \n",
    "        for word in self.w2i:\n",
    "            weights_matrix[self.w2i[word]] = embeddings[word]\n",
    "        self.embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        \n",
    "    def forward(self, input_src, prev_c, prev_h):\n",
    "        src_emb = self.embedding(input_src) # batch_size x src_length x emb_size\n",
    "        if self.opt.dropout > 0:\n",
    "            src_emb = self.dropout(src_emb)\n",
    "        prev_cy, prev_hy = self.lstm(src_emb, prev_c, prev_h)\n",
    "        return prev_cy, prev_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, opt, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.rnn_size\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.__initParameters()\n",
    "        \n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "         \n",
    "    def forward(self, prev_h):\n",
    "\n",
    "        if self.opt.dropout > 0:\n",
    "            prev_h = self.dropout(prev_h)\n",
    "        h2y = self.linear(prev_h)\n",
    "        pred = self.sigmoid(h2y)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tensors(data, w2i):\n",
    "    for sentence_report in data:\n",
    "        sentence_report.index_tensor = torch.zeros((1, len(sentence_report.preprocessed_sentence)), dtype=torch.long)\n",
    "        for idx, word in enumerate(sentence_report.preprocessed_sentence):\n",
    "            sentence_report.index_tensor[0][idx] = w2i[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(options):\n",
    "    if options.external_embedding is not None:\n",
    "        if os.path.isfile(os.path.join(options.saved_parameters_dir,\n",
    "                                       options.saved_prevectors)):\n",
    "            ext_embeddings, _ = IOUtils.load_embeddings_file(os.path.join(options.saved_parameters_dir,\n",
    "                                                                        options.saved_prevectors),\n",
    "                                                          \"pickle\",\n",
    "                                                          options.lower)\n",
    "            return ext_embeddings\n",
    "        else:\n",
    "            ext_embeddings, _ = IOUtils.load_embeddings_file(options.external_embedding,\n",
    "                                                          options.external_embedding_type,\n",
    "                                                          options.lower)\n",
    "            IOUtils.save_embeddings(os.path.join(options.saved_parameters_dir,\n",
    "                                                 options.saved_prevectors),\n",
    "                                    ext_embeddings)\n",
    "            return ext_embeddings\n",
    "    else:\n",
    "        raise Exception(\"external_embedding option is None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainItem(opt, sentence, encoder, classifier, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    for i in range(sentence.index_tensor.size(1)):\n",
    "        c, h = encoder(sentence.index_tensor[:, i], c, h)\n",
    "\n",
    "    pred = classifier(h)\n",
    "    loss = criterion(pred, torch.tensor([[sentence.mark]], dtype=torch.float))\n",
    "    loss.backward()\n",
    "    if opt.grad_clip != -1:\n",
    "        torch.nn.utils.clip_grad_value_(encoder.parameters(),opt.grad_clip)\n",
    "        torch.nn.utils.clip_grad_value_(classifier.parameters(),opt.grad_clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(opt, sentence, encoder, classifier):\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "\n",
    "    for i in range(sentence.index_tensor.size(1)):\n",
    "        c, h = encoder(sentence.index_tensor[:, i], c, h)\n",
    "\n",
    "    pred = classifier(h)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgumentParser()\n",
    "opt = TorchOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 226 ms, sys: 76 ms, total: 302 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ext_embeddings = load_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training vocabulary\n",
      "Saved vocab exists\n",
      "\n",
      "CPU times: user 2.53 ms, sys: 1.86 ms, total: 4.39 ms\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Extracting training vocabulary')\n",
    "w2i = IOUtils.load_vocab(args.train, args.train_file_type, args.saved_parameters_dir, args.saved_vocab_train,\n",
    "                                            args.external_embedding,\n",
    "                                            args.external_embedding_type,\n",
    "                                            args.stemmer,\n",
    "                                            args.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading row /home/huseyinalecakir/Security/data/acnet-data/ACNET_DATASET.csv \n",
      "Reading Train Sentences\n",
      "Loading completed\n",
      "CPU times: user 45.8 s, sys: 3.14 s, total: 49 s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = load_row_acnet(args.train, args.permission_type, args.stemmer, ext_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 222 ms, total: 2.32 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_index_tensors(sentences, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(opt, args, epoch_num, w2i, train_sentences, test_sentences, foldid):\n",
    "    encoder = Encoder(opt, w2i, ext_embeddings)\n",
    "    classifier = Classifier(opt, 1) \n",
    "    \n",
    "    params = list(encoder.parameters()) + list(classifier.parameters())\n",
    "    optimizer = optim.Adam(params) \n",
    "    optim_state = {\"learningRate\" : opt.learning_rate, \"alpha\" :  opt.decay_rate}\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    pr_scores = []\n",
    "    roc_scores = []\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"---Epoch {}---\\n\".format(epoch+1))\n",
    "        \n",
    "        print(\"Training...\")\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        for index, sentence in enumerate(train_data):\n",
    "            loss = trainItem(opt, sentence, encoder, classifier, optimizer, criterion)\n",
    "            if index != 0:\n",
    "                if index % opt.print_every == 0:\n",
    "                    print(\"Index {} Loss {}\".format(index,np.mean(losses[epoch*len(train_data)+index-opt.print_every:])))\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # Learning Rate Decay Optimization\n",
    "        if opt.learning_rate_decay < 1:\n",
    "            if epoch >= opt.learning_rate_decay_after:\n",
    "                decay_factor = opt.learning_rate_decay\n",
    "                optim_state[\"learningRate\"] = optim_state[\"learningRate\"] * decay_factor \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = optim_state[\"learningRate\"]\n",
    "\n",
    "        \n",
    "        print(\"Predicting..\")     \n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        predictions = []\n",
    "        gold = []\n",
    "        with torch.no_grad():\n",
    "            for index, sentence in enumerate(test_data):\n",
    "                pred = predict(opt, sentence, encoder, classifier)\n",
    "                predictions.append(pred)\n",
    "                gold.append(sentence.mark)\n",
    "\n",
    "        y_true = np.array(gold)\n",
    "        y_scores = np.array(predictions)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "        pr_auc = average_precision_score(y_true, y_scores)\n",
    "        pr_scores.append(pr_auc)\n",
    "        roc_scores.append(roc_auc)\n",
    "        print(\"Scores ROC {} PR {}\".format(roc_auc, pr_auc))\n",
    "        \n",
    "        #Save Model\n",
    "        model_save_dir = os.path.join(args.saved_parameters_dir, \"models\", \"fold{0}.epoch{1}.roc_auc{2:.2f}.prauc{3:.2f}.pt\".format(foldid, epoch, roc_auc, pr_auc))\n",
    "        print(model_save_dir)\n",
    "        if not os.path.exists(os.path.dirname(model_save_dir)):\n",
    "            os.makedirs(os.path.dirname(model_save_dir))\n",
    "        torch.save({\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch' : epoch,\n",
    "            'loss' : loss,\n",
    "            'foldid' : foldid,\n",
    "            'pr_auc' : pr_auc,\n",
    "            'roc_auc' : roc_auc\n",
    "            }, model_save_dir)\n",
    "    return roc_scores, pr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_predict(model_path, sentences):\n",
    "    documents = np.array(sentences)\n",
    "    documents = documents[:100]\n",
    "    kfold = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "    kfold_splits = kfold.split(documents)\n",
    "\n",
    "    encoder = Encoder(opt, w2i, ext_embeddings)\n",
    "    classifier = Classifier(opt, 1) \n",
    "    optimizer = optim.Adam(params) \n",
    "    optim_state = {\"learningRate\" : opt.learning_rate, \"alpha\" :  opt.decay_rate}\n",
    "    criterion = nn.BCELoss()  \n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "    classifier.load_state_dict(checkpoint[\"classifier\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    pr_auc = checkpoint[\"pr_auc\"]\n",
    "    foldid = checkpoint[\"foldid\"]\n",
    "    roc_auc = checkpoint[\"roc_auc\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    \n",
    "    train_indexes, test_indexes = kfold_splits[foldid] \n",
    "    test_data = documents[test_indexes]\n",
    "    with torch.no_grad():\n",
    "        for index, sentence in enumerate(test_data):\n",
    "            pred = predict(opt, sentence, encoder, classifier)\n",
    "            predictions.append(pred)\n",
    "            gold.append(sentence.mark)\n",
    "            y_true = np.array(gold)\n",
    "            \n",
    "    y_scores = np.array(predictions)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    pr_auc = average_precision_score(y_true, y_scores)\n",
    "    pr_scores.append(pr_auc)\n",
    "    roc_scores.append(roc_auc)\n",
    "    print(\"Scores ROC {} PR {}\".format(roc_auc, pr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD ID 1\n",
      "\n",
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 2500 Loss 0.16309403339517903\n",
      "Index 5000 Loss 0.17280172033768032\n",
      "Index 7500 Loss 0.11961089632874354\n",
      "Index 10000 Loss 0.048788565727579405\n",
      "Index 12500 Loss 0.07088449045346351\n",
      "Index 15000 Loss 0.09857503333240747\n",
      "Index 17500 Loss 0.1204223501475295\n",
      "Index 20000 Loss 0.08291337760443566\n",
      "Predicting..\n",
      "Scores ROC 0.950641217939103 PR 0.6892186017386039\n",
      "/home/huseyinalecakir/Security/data/saved-parameters/models/fold0.epoch0.roc_auc0.95.prauc0.69.pt\n",
      "Fold Results :\n",
      "\n",
      "ROC : [0.950641217939103]\n",
      "\n",
      "PR : [0.6892186017386039]\n",
      "\n",
      "\n",
      "FOLD ID 2\n",
      "\n",
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 2500 Loss 0.15682308713058585\n",
      "Index 5000 Loss 0.1792837899946433\n",
      "Index 7500 Loss 0.14996676094299183\n",
      "Index 10000 Loss 0.063108428824367\n",
      "Index 12500 Loss 0.0863180020816857\n",
      "Index 15000 Loss 0.09873142765311059\n",
      "Index 17500 Loss 0.13310094175324774\n",
      "Index 20000 Loss 0.08109340837106574\n",
      "Predicting..\n",
      "Scores ROC 0.9703038355431303 PR 0.7506134979570831\n",
      "/home/huseyinalecakir/Security/data/saved-parameters/models/fold1.epoch0.roc_auc0.97.prauc0.75.pt\n",
      "Fold Results :\n",
      "\n",
      "ROC : [0.9703038355431303]\n",
      "\n",
      "PR : [0.7506134979570831]\n",
      "\n",
      "\n",
      "FOLD ID 3\n",
      "\n",
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 2500 Loss 0.15558557817606075\n",
      "Index 5000 Loss 0.17262717257881305\n",
      "Index 7500 Loss 0.11327187118621078\n",
      "Index 10000 Loss 0.05884218210403342\n",
      "Index 12500 Loss 0.07928206881918014\n",
      "Index 15000 Loss 0.09937931614653207\n",
      "Index 17500 Loss 0.12628920523528941\n",
      "Index 20000 Loss 0.07747498432206922\n",
      "Predicting..\n",
      "Scores ROC 0.943056784599202 PR 0.5415153849143972\n",
      "/home/huseyinalecakir/Security/data/saved-parameters/models/fold2.epoch0.roc_auc0.94.prauc0.54.pt\n",
      "Fold Results :\n",
      "\n",
      "ROC : [0.943056784599202]\n",
      "\n",
      "PR : [0.5415153849143972]\n",
      "\n",
      "\n",
      "FOLD ID 4\n",
      "\n",
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 2500 Loss 0.1557077502957329\n",
      "Index 5000 Loss 0.16416342274975324\n",
      "Index 7500 Loss 0.09572771281343885\n",
      "Index 10000 Loss 0.04262901153392158\n",
      "Index 12500 Loss 0.09029875489459956\n",
      "Index 15000 Loss 0.09473087152726949\n",
      "Index 17500 Loss 0.12613471352574415\n",
      "Index 20000 Loss 0.07621060811528878\n",
      "Predicting..\n",
      "Scores ROC 0.9646517052767053 PR 0.6606508980931853\n",
      "/home/huseyinalecakir/Security/data/saved-parameters/models/fold3.epoch0.roc_auc0.96.prauc0.66.pt\n",
      "Fold Results :\n",
      "\n",
      "ROC : [0.9646517052767053]\n",
      "\n",
      "PR : [0.6606508980931853]\n",
      "\n",
      "\n",
      "FOLD ID 5\n",
      "\n",
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 2500 Loss 0.16374532422939647\n",
      "Index 5000 Loss 0.17447054573593196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2c7b7773e844>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(opt, args, epoch_num, w2i, train_sentences, test_sentences, foldid)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c6d467d8abca>\u001b[0m in \u001b[0;36mtrainItem\u001b[0;34m(opt, sentence, encoder, classifier, optimizer, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = np.array(sentences)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "kfold_splits = kfold.split(documents)\n",
    "\n",
    "\n",
    "chunkend_losses = []\n",
    "roc_scores = []\n",
    "pr_scores = [] \n",
    "for foldid, (train, test) in enumerate(kfold_splits):\n",
    "    print(\"\\nFOLD ID {}\\n\".format(foldid+1))\n",
    "    train_data = documents[train]\n",
    "    test_data = documents[test]\n",
    "\n",
    "    roc, pr = train_and_test(opt, args, 3, w2i, train_data, test_data, foldid)\n",
    "    print(\"Fold Results :\\n\")\n",
    "    print(\"ROC : {}\\n\".format(roc))\n",
    "    print(\"PR : {}\\n\".format(pr))\n",
    "    \n",
    "    roc_scores.append(roc)\n",
    "    pr_scores.append(pr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
